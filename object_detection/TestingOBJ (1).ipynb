{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a725d073",
   "metadata": {},
   "source": [
    "# Tensorflow 2.9.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e645db8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.9.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d31bb11",
   "metadata": {},
   "source": [
    "# Creating and interaction with \"data\" directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0842ce83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DATA_DIR = os.path.join(os.getcwd(), 'data')\n",
    "MODELS_DIR = os.path.join(DATA_DIR, 'models')\n",
    "for dir in [DATA_DIR, MODELS_DIR]:\n",
    "    if not os.path.exists(dir):\n",
    "        os.mkdir(dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c7ef2f",
   "metadata": {},
   "source": [
    "# Model downloading code \n",
    "\n",
    "You can download various models but here we will download ssd_mobilenet (SSD ResNet101 V1 FPN 640x640). Here in the Model zoo, you can find more models.  <https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md>_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d701625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import urllib.request\n",
    "\n",
    "# Download and extract model\n",
    "MODEL_DATE = '20200711'\n",
    "MODEL_NAME = 'ssd_mobilenet_v2_320x320_coco17_tpu-8'\n",
    "MODEL_TAR_FILENAME = MODEL_NAME + '.tar.gz'\n",
    "MODELS_DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/tf2/'\n",
    "MODEL_DOWNLOAD_LINK = MODELS_DOWNLOAD_BASE + MODEL_DATE + '/' + MODEL_TAR_FILENAME\n",
    "PATH_TO_MODEL_TAR = os.path.join(MODELS_DIR, MODEL_TAR_FILENAME)\n",
    "PATH_TO_CKPT = os.path.join(MODELS_DIR, os.path.join(MODEL_NAME, 'checkpoint/'))\n",
    "PATH_TO_CFG = os.path.join(MODELS_DIR, os.path.join(MODEL_NAME, 'pipeline.config'))\n",
    "if not os.path.exists(PATH_TO_CKPT):\n",
    "    print('Downloading model. This may take a while... ', end='')\n",
    "    urllib.request.urlretrieve(MODEL_DOWNLOAD_LINK, PATH_TO_MODEL_TAR)\n",
    "    tar_file = tarfile.open(PATH_TO_MODEL_TAR)\n",
    "    tar_file.extractall(MODELS_DIR)\n",
    "    tar_file.close()\n",
    "    os.remove(PATH_TO_MODEL_TAR)\n",
    "    print('Done')\n",
    "\n",
    "# Download labels file\n",
    "LABEL_FILENAME = 'mscoco_label_map.pbtxt'\n",
    "LABELS_DOWNLOAD_BASE = \\\n",
    "    'https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/data/'\n",
    "PATH_TO_LABELS = os.path.join(MODELS_DIR, os.path.join(MODEL_NAME, LABEL_FILENAME))\n",
    "if not os.path.exists(PATH_TO_LABELS):\n",
    "    print('Downloading label file... ', end='')\n",
    "    urllib.request.urlretrieve(LABELS_DOWNLOAD_BASE + LABEL_FILENAME, PATH_TO_LABELS)\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e35400a",
   "metadata": {},
   "source": [
    "# Then Load the SSD_MobileNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "790a0636",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'    # Suppress TensorFlow logging\n",
    "import tensorflow as tf\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "from object_detection.builders import model_builder\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')           # Suppress TensorFlow logging (2)\n",
    "\n",
    "# Enable GPU dynamic memory allocation\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "# Load pipeline config and build a detection model\n",
    "configs = config_util.get_configs_from_pipeline_file(PATH_TO_CFG)\n",
    "model_config = configs['model']\n",
    "detection_model = model_builder.build(model_config=model_config, is_training=False)\n",
    "\n",
    "# Restore checkpoint\n",
    "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "ckpt.restore(os.path.join(PATH_TO_CKPT, 'ckpt-0')).expect_partial()\n",
    "\n",
    "@tf.function\n",
    "def detect_fn(image):\n",
    "    \"\"\"Detect objects in image.\"\"\"\n",
    "\n",
    "    image, shapes = detection_model.preprocess(image)\n",
    "    prediction_dict = detection_model.predict(image, shapes)\n",
    "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
    "\n",
    "    return detections, prediction_dict, tf.reshape(shapes, [-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4f8885",
   "metadata": {},
   "source": [
    "# Now, Load label map data (for plotting)\n",
    "For instance, if the model predicts 10, then we know that this shows person.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7119b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: {'id': 1, 'name': 'person'}, 2: {'id': 2, 'name': 'bicycle'}, 3: {'id': 3, 'name': 'car'}, 4: {'id': 4, 'name': 'motorcycle'}, 5: {'id': 5, 'name': 'airplane'}, 6: {'id': 6, 'name': 'bus'}, 7: {'id': 7, 'name': 'train'}, 8: {'id': 8, 'name': 'truck'}, 9: {'id': 9, 'name': 'boat'}, 10: {'id': 10, 'name': 'traffic light'}, 11: {'id': 11, 'name': 'fire hydrant'}, 13: {'id': 13, 'name': 'stop sign'}, 14: {'id': 14, 'name': 'parking meter'}, 15: {'id': 15, 'name': 'bench'}, 16: {'id': 16, 'name': 'bird'}, 17: {'id': 17, 'name': 'cat'}, 18: {'id': 18, 'name': 'dog'}, 19: {'id': 19, 'name': 'horse'}, 20: {'id': 20, 'name': 'sheep'}, 21: {'id': 21, 'name': 'cow'}, 22: {'id': 22, 'name': 'elephant'}, 23: {'id': 23, 'name': 'bear'}, 24: {'id': 24, 'name': 'zebra'}, 25: {'id': 25, 'name': 'giraffe'}, 27: {'id': 27, 'name': 'backpack'}, 28: {'id': 28, 'name': 'umbrella'}, 31: {'id': 31, 'name': 'handbag'}, 32: {'id': 32, 'name': 'tie'}, 33: {'id': 33, 'name': 'suitcase'}, 34: {'id': 34, 'name': 'frisbee'}, 35: {'id': 35, 'name': 'skis'}, 36: {'id': 36, 'name': 'snowboard'}, 37: {'id': 37, 'name': 'sports ball'}, 38: {'id': 38, 'name': 'kite'}, 39: {'id': 39, 'name': 'baseball bat'}, 40: {'id': 40, 'name': 'baseball glove'}, 41: {'id': 41, 'name': 'skateboard'}, 42: {'id': 42, 'name': 'surfboard'}, 43: {'id': 43, 'name': 'tennis racket'}, 44: {'id': 44, 'name': 'bottle'}, 46: {'id': 46, 'name': 'wine glass'}, 47: {'id': 47, 'name': 'cup'}, 48: {'id': 48, 'name': 'fork'}, 49: {'id': 49, 'name': 'knife'}, 50: {'id': 50, 'name': 'spoon'}, 51: {'id': 51, 'name': 'bowl'}, 52: {'id': 52, 'name': 'banana'}, 53: {'id': 53, 'name': 'apple'}, 54: {'id': 54, 'name': 'sandwich'}, 55: {'id': 55, 'name': 'orange'}, 56: {'id': 56, 'name': 'broccoli'}, 57: {'id': 57, 'name': 'carrot'}, 58: {'id': 58, 'name': 'hot dog'}, 59: {'id': 59, 'name': 'pizza'}, 60: {'id': 60, 'name': 'donut'}, 61: {'id': 61, 'name': 'cake'}, 62: {'id': 62, 'name': 'chair'}, 63: {'id': 63, 'name': 'couch'}, 64: {'id': 64, 'name': 'potted plant'}, 65: {'id': 65, 'name': 'bed'}, 67: {'id': 67, 'name': 'dining table'}, 70: {'id': 70, 'name': 'toilet'}, 72: {'id': 72, 'name': 'tv'}, 73: {'id': 73, 'name': 'laptop'}, 74: {'id': 74, 'name': 'mouse'}, 75: {'id': 75, 'name': 'remote'}, 76: {'id': 76, 'name': 'keyboard'}, 77: {'id': 77, 'name': 'cell phone'}, 78: {'id': 78, 'name': 'microwave'}, 79: {'id': 79, 'name': 'oven'}, 80: {'id': 80, 'name': 'toaster'}, 81: {'id': 81, 'name': 'sink'}, 82: {'id': 82, 'name': 'refrigerator'}, 84: {'id': 84, 'name': 'book'}, 85: {'id': 85, 'name': 'clock'}, 86: {'id': 86, 'name': 'vase'}, 87: {'id': 87, 'name': 'scissors'}, 88: {'id': 88, 'name': 'teddy bear'}, 89: {'id': 89, 'name': 'hair drier'}, 90: {'id': 90, 'name': 'toothbrush'}}\n",
      "{'person': 1, 'bicycle': 2, 'car': 3, 'motorcycle': 4, 'airplane': 5, 'bus': 6, 'train': 7, 'truck': 8, 'boat': 9, 'traffic light': 10, 'fire hydrant': 11, 'stop sign': 13, 'parking meter': 14, 'bench': 15, 'bird': 16, 'cat': 17, 'dog': 18, 'horse': 19, 'sheep': 20, 'cow': 21, 'elephant': 22, 'bear': 23, 'zebra': 24, 'giraffe': 25, 'backpack': 27, 'umbrella': 28, 'handbag': 31, 'tie': 32, 'suitcase': 33, 'frisbee': 34, 'skis': 35, 'snowboard': 36, 'sports ball': 37, 'kite': 38, 'baseball bat': 39, 'baseball glove': 40, 'skateboard': 41, 'surfboard': 42, 'tennis racket': 43, 'bottle': 44, 'wine glass': 46, 'cup': 47, 'fork': 48, 'knife': 49, 'spoon': 50, 'bowl': 51, 'banana': 52, 'apple': 53, 'sandwich': 54, 'orange': 55, 'broccoli': 56, 'carrot': 57, 'hot dog': 58, 'pizza': 59, 'donut': 60, 'cake': 61, 'chair': 62, 'couch': 63, 'potted plant': 64, 'bed': 65, 'dining table': 67, 'toilet': 70, 'tv': 72, 'laptop': 73, 'mouse': 74, 'remote': 75, 'keyboard': 76, 'cell phone': 77, 'microwave': 78, 'oven': 79, 'toaster': 80, 'sink': 81, 'refrigerator': 82, 'book': 84, 'clock': 85, 'vase': 86, 'scissors': 87, 'teddy bear': 88, 'hair drier': 89, 'toothbrush': 90}\n"
     ]
    }
   ],
   "source": [
    "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS,\n",
    "                                                                    use_display_name=True)\n",
    "name_to_id = {value['name']: key for key, value in category_index.items()}\n",
    "print(category_index)\n",
    "print(name_to_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d0b413",
   "metadata": {},
   "source": [
    "# Define the video stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54857efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import speech_recognition as sr\n",
    "import pyttsx3 \n",
    "import threading\n",
    "\n",
    "# Function to convert text to\n",
    "# speech\n",
    "def SpeakText(command):\n",
    "    \n",
    "    # Initialize the engine\n",
    "    engine = pyttsx3.init()\n",
    "    engine.say(command) \n",
    "    engine.runAndWait()\n",
    "\n",
    "detected_objects = []\n",
    "scores = []\n",
    "\n",
    "\n",
    "def recognize_speech():\n",
    "    global detected_objects\n",
    "    global scores\n",
    "\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"Adjusting for noise... Please wait.\")\n",
    "        print(\"Listening...\")\n",
    "        while True:\n",
    "            try:\n",
    "                r.adjust_for_ambient_noise(source)\n",
    "                audio = r.listen(source)\n",
    "                MyText = r.recognize_google(audio).lower()\n",
    "                print(f\"Did you say: {MyText}\")\n",
    "                if \"what can you see\" in MyText:\n",
    "                    if detected_objects:\n",
    "                        response = f\"I can see: {', '.join(detected_objects)}\"\n",
    "                    else:\n",
    "                        response = \"I can't see anything clearly.\"\n",
    "                    print(response)\n",
    "                    SpeakText(response)\n",
    "                if \"find\" in MyText:\n",
    "                    list1=MyText.split(\" \")\n",
    "                    obj1=list1[-1]\n",
    "                    response = f\"Looking for: {obj1}\"\n",
    "                    print(response)\n",
    "                    SpeakText(response)\n",
    "                    if obj1 in name_to_id and name_to_id[obj1] in classes:\n",
    "                        max_score = 0\n",
    "                        for i in range(len(classes)):\n",
    "                            if classes[i] == name_to_id[obj1] and scores[i] > max_score:\n",
    "                                max_score = scores[i]\n",
    "                        if max_score > 0.5:\n",
    "                            response = \"I can see it.\"\n",
    "                        else:\n",
    "                            response = f\"I cannot see what you want. I can see: {', '.join(detected_objects)}\"\n",
    "                    else:\n",
    "                        response = f\"I cannot see {obj1}. I can see: {', '.join(detected_objects)}\"\n",
    "                    \n",
    "                    print(response)\n",
    "                    SpeakText(response)\n",
    "\n",
    "                \n",
    "            except sr.UnknownValueError:\n",
    "                print(\"Could not understand audio\")\n",
    "            except sr.RequestError as e:\n",
    "                print(f\"API request error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff84841",
   "metadata": {},
   "source": [
    "# Run the following code for webcam (Credit goes to Krish Naik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a54f776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting for noise... Please wait.\n",
      "Listening...\n",
      "Did you say: find me a person\n",
      "Looking for: person\n",
      "I can see it.\n",
      "Did you say: find me a person\n",
      "Looking for: person\n",
      "I cannot see what you want. I can see: \n",
      "Did you say: find me a person find me a book\n",
      "Looking for: book\n",
      "I cannot see what you want. I can see: person\n",
      "Did you say: find me a person\n",
      "Looking for: person\n",
      "I can see it.\n",
      "Did you say: find a person\n",
      "Looking for: person\n",
      "I cannot see what you want. I can see: \n",
      "Did you say: find me a book\n",
      "Looking for: book\n",
      "I can see it.\n",
      "Did you say: find me a book\n",
      "Looking for: book\n",
      "I cannot see book. I can see: person\n",
      "Did you say: okay it's so good it's actually incredible chat\n",
      "Did you say: chat\n",
      "Did you say: let's get rid of the cannot understand\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\PIL\\ImageFont.py:879\u001b[0m, in \u001b[0;36mtruetype\u001b[1;34m(font, size, index, encoding, layout_engine)\u001b[0m\n\u001b[0;32m    878\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfreetype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfont\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\PIL\\ImageFont.py:876\u001b[0m, in \u001b[0;36mtruetype.<locals>.freetype\u001b[1;34m(font)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfreetype\u001b[39m(font: StrOrBytesPath \u001b[38;5;241m|\u001b[39m BinaryIO) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FreeTypeFont:\n\u001b[1;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFreeTypeFont\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfont\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayout_engine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\PIL\\ImageFont.py:284\u001b[0m, in \u001b[0;36mFreeTypeFont.__init__\u001b[1;34m(self, font, size, index, encoding, layout_engine)\u001b[0m\n\u001b[0;32m    283\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 284\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfont \u001b[38;5;241m=\u001b[39m \u001b[43mcore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetfont\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfont\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayout_engine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayout_engine\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mOSError\u001b[0m: cannot open resource",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 49\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[38;5;66;03m#detected_objects.append((detections['detection_boxes'][0].numpy(),\u001b[39;00m\n\u001b[0;32m     42\u001b[0m       \u001b[38;5;66;03m#(detections['detection_classes'][0].numpy() + label_id_offset).astype(int),\u001b[39;00m\n\u001b[0;32m     43\u001b[0m       \u001b[38;5;66;03m#detections['detection_scores'][0].numpy()))\u001b[39;00m\n\u001b[0;32m     47\u001b[0m image_np_with_detections \u001b[38;5;241m=\u001b[39m image_np\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m---> 49\u001b[0m \u001b[43mviz_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisualize_boxes_and_labels_on_image_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m      \u001b[49m\u001b[43mimage_np_with_detections\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdetections\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdetection_boxes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m      \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdetections\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdetection_classes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabel_id_offset\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdetections\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdetection_scores\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcategory_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m      \u001b[49m\u001b[43muse_normalized_coordinates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmax_boxes_to_draw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmin_score_thresh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m      \u001b[49m\u001b[43magnostic_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Display output\u001b[39;00m\n\u001b[0;32m     61\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject detection\u001b[39m\u001b[38;5;124m'\u001b[39m, cv2\u001b[38;5;241m.\u001b[39mresize(image_np_with_detections, (\u001b[38;5;241m800\u001b[39m, \u001b[38;5;241m600\u001b[39m)))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\object_detection\\utils\\visualization_utils.py:1251\u001b[0m, in \u001b[0;36mvisualize_boxes_and_labels_on_image_array\u001b[1;34m(image, boxes, classes, scores, category_index, instance_masks, instance_boundaries, keypoints, keypoint_scores, keypoint_edges, track_ids, use_normalized_coordinates, max_boxes_to_draw, min_score_thresh, agnostic_mode, line_thickness, mask_alpha, groundtruth_box_visualization_color, skip_boxes, skip_scores, skip_labels, skip_track_ids)\u001b[0m\n\u001b[0;32m   1244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m instance_boundaries \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1245\u001b[0m   draw_mask_on_image_array(\n\u001b[0;32m   1246\u001b[0m       image,\n\u001b[0;32m   1247\u001b[0m       box_to_instance_boundaries_map[box],\n\u001b[0;32m   1248\u001b[0m       color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1249\u001b[0m       alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m\n\u001b[0;32m   1250\u001b[0m   )\n\u001b[1;32m-> 1251\u001b[0m \u001b[43mdraw_bounding_box_on_image_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mymin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mymax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxmax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthickness\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mskip_boxes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline_thickness\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1259\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisplay_str_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbox_to_display_str_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbox\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1260\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_normalized_coordinates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_normalized_coordinates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keypoints \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1262\u001b[0m   keypoint_scores_for_box \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\object_detection\\utils\\visualization_utils.py:160\u001b[0m, in \u001b[0;36mdraw_bounding_box_on_image_array\u001b[1;34m(image, ymin, xmin, ymax, xmax, color, thickness, display_str_list, use_normalized_coordinates)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Adds a bounding box to an image (numpy array).\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \n\u001b[0;32m    142\u001b[0m \u001b[38;5;124;03mBounding box coordinates can be specified in either absolute (pixel) or\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03m    coordinates as absolute.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    159\u001b[0m image_pil \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(np\u001b[38;5;241m.\u001b[39muint8(image))\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 160\u001b[0m \u001b[43mdraw_bounding_box_on_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_pil\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mymin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxmin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mymax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxmax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mthickness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisplay_str_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m                           \u001b[49m\u001b[43muse_normalized_coordinates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m np\u001b[38;5;241m.\u001b[39mcopyto(image, np\u001b[38;5;241m.\u001b[39marray(image_pil))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\object_detection\\utils\\visualization_utils.py:212\u001b[0m, in \u001b[0;36mdraw_bounding_box_on_image\u001b[1;34m(image, ymin, xmin, ymax, xmax, color, thickness, display_str_list, use_normalized_coordinates)\u001b[0m\n\u001b[0;32m    207\u001b[0m   draw\u001b[38;5;241m.\u001b[39mline([(left, top), (left, bottom), (right, bottom), (right, top),\n\u001b[0;32m    208\u001b[0m              (left, top)],\n\u001b[0;32m    209\u001b[0m             width\u001b[38;5;241m=\u001b[39mthickness,\n\u001b[0;32m    210\u001b[0m             fill\u001b[38;5;241m=\u001b[39mcolor)\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 212\u001b[0m   font \u001b[38;5;241m=\u001b[39m \u001b[43mImageFont\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtruetype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43marial.ttf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m24\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m:\n\u001b[0;32m    214\u001b[0m   font \u001b[38;5;241m=\u001b[39m ImageFont\u001b[38;5;241m.\u001b[39mload_default()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\PIL\\ImageFont.py:919\u001b[0m, in \u001b[0;36mtruetype\u001b[1;34m(font, size, index, encoding, layout_engine)\u001b[0m\n\u001b[0;32m    917\u001b[0m first_font_with_a_different_extension \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m directory \u001b[38;5;129;01min\u001b[39;00m dirs:\n\u001b[1;32m--> 919\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m walkroot, walkdir, walkfilenames \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mwalk(directory):\n\u001b[0;32m    920\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m walkfilename \u001b[38;5;129;01min\u001b[39;00m walkfilenames:\n\u001b[0;32m    921\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mand\u001b[39;00m walkfilename \u001b[38;5;241m==\u001b[39m ttf_filename:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\os.py:357\u001b[0m, in \u001b[0;36m_walk\u001b[1;34m(top, topdown, onerror, followlinks)\u001b[0m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;66;03m# We may not have read permission for top, in which case we can't\u001b[39;00m\n\u001b[0;32m    350\u001b[0m \u001b[38;5;66;03m# get a list of the files the directory contains.  os.walk\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# always suppressed the exception then, rather than blow up for a\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# minor reason when (say) a thousand readable directories are still\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;66;03m# left to visit.  That logic is copied here.\u001b[39;00m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;66;03m# Note that scandir is global in this module due\u001b[39;00m\n\u001b[0;32m    356\u001b[0m     \u001b[38;5;66;03m# to earlier import-*.\u001b[39;00m\n\u001b[1;32m--> 357\u001b[0m     scandir_it \u001b[38;5;241m=\u001b[39m \u001b[43mscandir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m onerror \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-3:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Sumayya\\AppData\\Local\\Temp\\ipykernel_13808\\1630875376.py\", line 30, in recognize_speech\n",
      "  File \"C:\\Users\\Sumayya\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\speech_recognition\\recognizers\\google.py\", line 262, in recognize_legacy\n",
      "    return output_parser.parse(response_text)\n",
      "  File \"C:\\Users\\Sumayya\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\speech_recognition\\recognizers\\google.py\", line 134, in parse\n",
      "    actual_result = self.convert_to_result(response_text)\n",
      "  File \"C:\\Users\\Sumayya\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\speech_recognition\\recognizers\\google.py\", line 183, in convert_to_result\n",
      "    raise UnknownValueError()\n",
      "speech_recognition.exceptions.UnknownValueError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Sumayya\\anaconda3\\envs\\tensorflow\\lib\\threading.py\", line 980, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\Sumayya\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"C:\\Users\\Sumayya\\anaconda3\\envs\\tensorflow\\lib\\threading.py\", line 917, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\Sumayya\\AppData\\Local\\Temp\\ipykernel_13808\\1630875376.py\", line 61, in recognize_speech\n",
      "NameError: name 'sr' is not defined\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Initialize the recognizer \n",
    "r = sr.Recognizer() \n",
    "\n",
    "# Start speech recognition in a separate thread\n",
    "speech_thread = threading.Thread(target=recognize_speech, daemon=True)\n",
    "speech_thread.start()\n",
    "\n",
    "while True:\n",
    "    # Read frame from camera\n",
    "    ret, image_np = cap.read()\n",
    "\n",
    "    # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
    "    image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "\n",
    "    # Things to try:\n",
    "    # Flip horizontally\n",
    "    # image_np = np.fliplr(image_np).copy()\n",
    "\n",
    "    # Convert image to grayscale\n",
    "    # image_np = np.tile(\n",
    "    #     np.mean(image_np, 2, keepdims=True), (1, 1, 3)).astype(np.uint8)\n",
    "\n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "    detections, predictions_dict, shapes = detect_fn(input_tensor)\n",
    "     # âœ… Extract detected objects\n",
    "    scores = detections['detection_scores'][0].numpy()\n",
    "    #print(scores)\n",
    "    #print(len(scores))\n",
    "    classes = (detections['detection_classes'][0].numpy() + 1).astype(int)\n",
    "    label_id_offset = 1\n",
    "    detected_objects.clear()\n",
    "    for i in range(len(scores)):\n",
    "        if scores[i] > 0.5:  # Only consider objects with confidence > 50%\n",
    "            new_object = category_index.get(classes[i], {'name': 'unknown'})['name']\n",
    "            if new_object not in detected_objects:\n",
    "                detected_objects.append(new_object)\n",
    "            #detected_objects.append((detections['detection_boxes'][0].numpy(),\n",
    "          #(detections['detection_classes'][0].numpy() + label_id_offset).astype(int),\n",
    "          #detections['detection_scores'][0].numpy()))\n",
    "\n",
    "    \n",
    "    \n",
    "    image_np_with_detections = image_np.copy()\n",
    "    \n",
    "    viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "          image_np_with_detections,\n",
    "          detections['detection_boxes'][0].numpy(),\n",
    "          (detections['detection_classes'][0].numpy() + label_id_offset).astype(int),\n",
    "          detections['detection_scores'][0].numpy(),\n",
    "          category_index,\n",
    "          use_normalized_coordinates=True,\n",
    "          max_boxes_to_draw=200,\n",
    "          min_score_thresh=0.6,\n",
    "          agnostic_mode=False)\n",
    "\n",
    "    # Display output\n",
    "    cv2.imshow('object detection', cv2.resize(image_np_with_detections, (800, 600)))\n",
    "\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680ef3e2",
   "metadata": {},
   "source": [
    "# Run the following code for video. \n",
    "Put the video (for example test2.mp4) in object_detection folder (\\models\\research\\object_detection). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ac5948",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
